---
layout: post
image: /images/japan.png
title:  "Optimization and Evaluation of Multi-layer Neural Networks: Exploring Regularization, Learning Rates, and Topologies"
authors: "<strong>Yi Li</strong>, Weiyue Li, Linghang Kong"
info: "Deep Learning, UCSD"
date:   2022-02-15 00:00:00 +00:00
code: https://github.com/jerryli1019/MLP
paper: https://github.com/jerryli1019/MLP/blob/main/report.pdf
categories: others-dl
---
In this study, we implemented a multi-layer neural network that features forward and backward propagation, several regularization techniques, and momentum-based optimization. Our goal was to classify Japanese Hiragana handwritten characters from the KMNIST dataset using a softmax output layer. We employed one-fold cross-validation to assess the model's performance and incorporated various regularization methods. Our most effective model utilized ReLU activation functions and achieved an accuracy of 0.8688. After making further adjustments to the architecture, including changes to the layer count and hidden units, we observed a test set accuracy of 0.8626.